# Build the docker image
docker build --build-arg python_version=3.11 -t flexflow .

# Run the docker container
docker run --gpus all -it     -v /home/yingyih/workspace/flash-attn-integration/flash_attention_lib:/workspace/flash_attention_lib  --user $(id -u):$(id -g)   flexflow

# Ensure torch with cuda support
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"

# Install flash-attn
pip install --no-build-isolation flash-attn --no-cache-dir

# Verify flash-attn installation
# Expect: /opt/conda/lib/python3.11/site-packages/flash_attn/__init__.py
python -c "import flash_attn; print(flash_attn.__file__)"

# Find attn.so
find /opt/conda -name "flash_attn*.so"
# /opt/conda/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so

# Set LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/opt/conda/lib/python3.11/site-packages/torch/lib:$LD_LIBRARY_PATH

# command to test
cd workspace/flash_attention_lib/build/
rm -rf * && cmake .. && make -j && ./test_run_mha_bwd && ./test_run_mha_fwd && ./test_mha_fwd_bwd

# attention:
Using version >= 2.7.4.post1, the tested functions will be under flash namespace.
